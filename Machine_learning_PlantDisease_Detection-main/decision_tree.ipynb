{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":182633,"sourceType":"datasetVersion","datasetId":78313}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pylab as plt\nimport os\nimport shutil\nimport cv2\nfrom sklearn import tree\nfrom sklearn.metrics import confusion_matrix\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T02:07:16.011698Z","iopub.execute_input":"2025-01-23T02:07:16.012106Z","iopub.status.idle":"2025-01-23T02:07:17.510483Z","shell.execute_reply.started":"2025-01-23T02:07:16.012077Z","shell.execute_reply":"2025-01-23T02:07:17.509715Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n\n\n# Paths for the dataset\ntrain_path = \"/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train\"\nvalidation_path = \"/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid\"\noutput_train_path = \"/kaggle/working/reduced_dataset/train\"\noutput_validation_path = \"/kaggle/working/reduced_dataset/valid\"\n\n# # Number of images per class\ntrain_samples = 1000\nvalidation_samples = 300\n\n# # Function to sample and copy images\ndef create_subset(input_path, output_path, samples_per_class):\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n    for class_name in os.listdir(input_path):\n        class_dir = os.path.join(input_path, class_name)\n        if os.path.isdir(class_dir):\n            images = os.listdir(class_dir)\n            \n            selected_images = images[:samples_per_class]\n            class_output_dir = os.path.join(output_path, class_name)\n            os.makedirs(class_output_dir, exist_ok=True)\n            for image in selected_images:\n                src = os.path.join(class_dir, image)\n                dst = os.path.join(class_output_dir, image)\n                shutil.copy(src, dst)\n            print(f\"Processed class '{class_name}' with {samples_per_class} images.\")\n\n# Create subsets for training and validation\ncreate_subset(train_path, output_train_path, train_samples)\ncreate_subset(validation_path, output_validation_path, validation_samples)\ncreate_subset(\"/kaggle/input/new-plant-diseases-dataset/test\",\"/kaggle/working/reduced_dataset/test\",33)\nprint(\"Subset creation completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T02:07:47.300723Z","iopub.execute_input":"2025-01-23T02:07:47.301079Z","iopub.status.idle":"2025-01-23T02:12:06.106243Z","shell.execute_reply.started":"2025-01-23T02:07:47.301050Z","shell.execute_reply":"2025-01-23T02:12:06.105337Z"}},"outputs":[{"name":"stdout","text":"Processed class 'Tomato___Late_blight' with 1000 images.\nProcessed class 'Tomato___healthy' with 1000 images.\nProcessed class 'Grape___healthy' with 1000 images.\nProcessed class 'Orange___Haunglongbing_(Citrus_greening)' with 1000 images.\nProcessed class 'Soybean___healthy' with 1000 images.\nProcessed class 'Squash___Powdery_mildew' with 1000 images.\nProcessed class 'Potato___healthy' with 1000 images.\nProcessed class 'Corn_(maize)___Northern_Leaf_Blight' with 1000 images.\nProcessed class 'Tomato___Early_blight' with 1000 images.\nProcessed class 'Tomato___Septoria_leaf_spot' with 1000 images.\nProcessed class 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot' with 1000 images.\nProcessed class 'Strawberry___Leaf_scorch' with 1000 images.\nProcessed class 'Peach___healthy' with 1000 images.\nProcessed class 'Apple___Apple_scab' with 1000 images.\nProcessed class 'Tomato___Tomato_Yellow_Leaf_Curl_Virus' with 1000 images.\nProcessed class 'Tomato___Bacterial_spot' with 1000 images.\nProcessed class 'Apple___Black_rot' with 1000 images.\nProcessed class 'Blueberry___healthy' with 1000 images.\nProcessed class 'Cherry_(including_sour)___Powdery_mildew' with 1000 images.\nProcessed class 'Peach___Bacterial_spot' with 1000 images.\nProcessed class 'Apple___Cedar_apple_rust' with 1000 images.\nProcessed class 'Tomato___Target_Spot' with 1000 images.\nProcessed class 'Pepper,_bell___healthy' with 1000 images.\nProcessed class 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)' with 1000 images.\nProcessed class 'Potato___Late_blight' with 1000 images.\nProcessed class 'Tomato___Tomato_mosaic_virus' with 1000 images.\nProcessed class 'Strawberry___healthy' with 1000 images.\nProcessed class 'Apple___healthy' with 1000 images.\nProcessed class 'Grape___Black_rot' with 1000 images.\nProcessed class 'Potato___Early_blight' with 1000 images.\nProcessed class 'Cherry_(including_sour)___healthy' with 1000 images.\nProcessed class 'Corn_(maize)___Common_rust_' with 1000 images.\nProcessed class 'Grape___Esca_(Black_Measles)' with 1000 images.\nProcessed class 'Raspberry___healthy' with 1000 images.\nProcessed class 'Tomato___Leaf_Mold' with 1000 images.\nProcessed class 'Tomato___Spider_mites Two-spotted_spider_mite' with 1000 images.\nProcessed class 'Pepper,_bell___Bacterial_spot' with 1000 images.\nProcessed class 'Corn_(maize)___healthy' with 1000 images.\nProcessed class 'Tomato___Late_blight' with 300 images.\nProcessed class 'Tomato___healthy' with 300 images.\nProcessed class 'Grape___healthy' with 300 images.\nProcessed class 'Orange___Haunglongbing_(Citrus_greening)' with 300 images.\nProcessed class 'Soybean___healthy' with 300 images.\nProcessed class 'Squash___Powdery_mildew' with 300 images.\nProcessed class 'Potato___healthy' with 300 images.\nProcessed class 'Corn_(maize)___Northern_Leaf_Blight' with 300 images.\nProcessed class 'Tomato___Early_blight' with 300 images.\nProcessed class 'Tomato___Septoria_leaf_spot' with 300 images.\nProcessed class 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot' with 300 images.\nProcessed class 'Strawberry___Leaf_scorch' with 300 images.\nProcessed class 'Peach___healthy' with 300 images.\nProcessed class 'Apple___Apple_scab' with 300 images.\nProcessed class 'Tomato___Tomato_Yellow_Leaf_Curl_Virus' with 300 images.\nProcessed class 'Tomato___Bacterial_spot' with 300 images.\nProcessed class 'Apple___Black_rot' with 300 images.\nProcessed class 'Blueberry___healthy' with 300 images.\nProcessed class 'Cherry_(including_sour)___Powdery_mildew' with 300 images.\nProcessed class 'Peach___Bacterial_spot' with 300 images.\nProcessed class 'Apple___Cedar_apple_rust' with 300 images.\nProcessed class 'Tomato___Target_Spot' with 300 images.\nProcessed class 'Pepper,_bell___healthy' with 300 images.\nProcessed class 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)' with 300 images.\nProcessed class 'Potato___Late_blight' with 300 images.\nProcessed class 'Tomato___Tomato_mosaic_virus' with 300 images.\nProcessed class 'Strawberry___healthy' with 300 images.\nProcessed class 'Apple___healthy' with 300 images.\nProcessed class 'Grape___Black_rot' with 300 images.\nProcessed class 'Potato___Early_blight' with 300 images.\nProcessed class 'Cherry_(including_sour)___healthy' with 300 images.\nProcessed class 'Corn_(maize)___Common_rust_' with 300 images.\nProcessed class 'Grape___Esca_(Black_Measles)' with 300 images.\nProcessed class 'Raspberry___healthy' with 300 images.\nProcessed class 'Tomato___Leaf_Mold' with 300 images.\nProcessed class 'Tomato___Spider_mites Two-spotted_spider_mite' with 300 images.\nProcessed class 'Pepper,_bell___Bacterial_spot' with 300 images.\nProcessed class 'Corn_(maize)___healthy' with 300 images.\nProcessed class 'test' with 33 images.\nSubset creation completed.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"training_data = os.listdir(\"/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train\")\ntest_data=glob('/kaggle/input/new-plant-diseases-dataset/test/test/*.JPG')\nvalidation_data=os.listdir('/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T22:50:49.330458Z","iopub.execute_input":"2025-01-22T22:50:49.330993Z","iopub.status.idle":"2025-01-22T22:50:49.351377Z","shell.execute_reply.started":"2025-01-22T22:50:49.330940Z","shell.execute_reply":"2025-01-22T22:50:49.350239Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def extract_features(image_path):\n    \n    image = cv2.imread(image_path)\n  \n    image = cv2.resize(image, (128, 128))\n    \n    hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    hist = cv2.normalize(hist, hist).flatten()\n\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    lbp = cv2.Laplacian(gray, cv2.CV_64F).var()  \n    \n    features = np.hstack([hist, lbp])\n    return features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_images_path=dict()\nfor class_path in training_data:\n    \n    images_path=[]\n    \n    for class_image in os.listdir(\"/kaggle/working/reduced_dataset/train/\"+class_path):\n        images_path.append(\"/kaggle/working/reduced_dataset/train/\"+class_path+\"/\"+class_image)\n        \n    training_images_path.update({class_path:images_path})\n        \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_images_path=dict()\nfor class_path in validation_data:\n    images_path=[]\n    \n    for class_image in os.listdir(\"/kaggle/working/reduced_dataset/valid/\"+class_path):\n        images_path.append(\"/kaggle/working/reduced_dataset/valid/\"+class_path+\"/\"+class_image)\n    \n    validation_images_path.update({class_path:images_path})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T23:34:08.515293Z","iopub.execute_input":"2025-01-22T23:34:08.515645Z","iopub.status.idle":"2025-01-22T23:34:08.535946Z","shell.execute_reply.started":"2025-01-22T23:34:08.515618Z","shell.execute_reply":"2025-01-22T23:34:08.534857Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"prepared_training_data=dict()\nfor class_element in training_images_path:\n    features=[]\n    for image_path in training_images_path.get(class_element):\n        features.append(extract_features(image_path))\n    prepared_training_data.update({class_element:features})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T23:34:10.699453Z","iopub.execute_input":"2025-01-22T23:34:10.699889Z","iopub.status.idle":"2025-01-22T23:34:48.026308Z","shell.execute_reply.started":"2025-01-22T23:34:10.699857Z","shell.execute_reply":"2025-01-22T23:34:48.025212Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"prepared_validation_data=dict()\nfor class_element in validation_images_path:\n    features=[]\n    for image_path in validation_images_path.get(class_element):\n        \n        features.append(extract_features(image_path))\n    prepared_validation_data.update({class_element:features})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T23:34:59.989049Z","iopub.execute_input":"2025-01-22T23:34:59.989387Z","iopub.status.idle":"2025-01-22T23:35:10.959222Z","shell.execute_reply.started":"2025-01-22T23:34:59.989363Z","shell.execute_reply":"2025-01-22T23:35:10.958079Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"x=[]\ny=[]\ni=0\nfor class_name, feature_list in prepared_training_data.items():\n    for features in feature_list:\n        x.append(features)  # Add the feature vector\n        y.append(i)\n    i=i+1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T23:35:12.764965Z","iopub.execute_input":"2025-01-22T23:35:12.765343Z","iopub.status.idle":"2025-01-22T23:35:12.798319Z","shell.execute_reply.started":"2025-01-22T23:35:12.765319Z","shell.execute_reply":"2025-01-22T23:35:12.797141Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"clf=tree.DecisionTreeRegressor()\n\nclf.fit(x,y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T23:35:17.813371Z","iopub.execute_input":"2025-01-22T23:35:17.813814Z","iopub.status.idle":"2025-01-22T23:35:23.904298Z","shell.execute_reply.started":"2025-01-22T23:35:17.813762Z","shell.execute_reply":"2025-01-22T23:35:23.903210Z"}},"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"DecisionTreeRegressor()","text/html":"<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"x_test=[]\ny_test=[]\ni=0\nfor class_name, feature_list in prepared_validation_data.items():\n    for features in feature_list:\n        x_test.append(features)  # Add the feature vector\n        y_test.append(i)\n    i=i+1\ny_pred=clf.predict(x_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T23:36:20.545616Z","iopub.execute_input":"2025-01-22T23:36:20.546026Z","iopub.status.idle":"2025-01-22T23:36:20.573678Z","shell.execute_reply.started":"2025-01-22T23:36:20.545997Z","shell.execute_reply":"2025-01-22T23:36:20.572538Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nprint(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T23:36:25.412957Z","iopub.execute_input":"2025-01-22T23:36:25.413331Z","iopub.status.idle":"2025-01-22T23:36:25.455077Z","shell.execute_reply.started":"2025-01-22T23:36:25.413302Z","shell.execute_reply":"2025-01-22T23:36:25.453745Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.64      0.63      0.64       300\n           1       0.90      0.93      0.92       300\n           2       0.76      0.82      0.79       300\n           3       0.76      0.71      0.74       300\n           4       0.71      0.63      0.67       300\n           5       0.80      0.72      0.76       300\n           6       0.75      0.82      0.79       300\n           7       0.74      0.75      0.75       300\n           8       0.71      0.72      0.72       300\n           9       0.61      0.57      0.59       300\n          10       0.65      0.69      0.67       300\n          11       0.84      0.78      0.81       300\n          12       0.88      0.91      0.89       300\n          13       0.59      0.58      0.58       300\n          14       0.69      0.66      0.68       300\n          15       0.71      0.66      0.68       300\n          16       0.75      0.71      0.73       300\n          17       0.71      0.65      0.68       300\n          18       0.76      0.79      0.78       300\n          19       0.61      0.55      0.58       300\n          20       0.73      0.74      0.73       300\n          21       0.69      0.69      0.69       300\n          22       0.56      0.56      0.56       300\n          23       0.82      0.81      0.82       300\n          24       0.65      0.78      0.71       300\n          25       0.84      0.81      0.83       300\n          26       0.70      0.68      0.69       300\n          27       0.72      0.76      0.74       300\n          28       0.66      0.68      0.67       300\n          29       0.88      0.88      0.88       300\n          30       0.83      0.88      0.86       300\n          31       0.94      0.95      0.94       300\n          32       0.90      0.86      0.88       300\n          33       0.70      0.75      0.72       300\n          34       0.82      0.84      0.83       300\n          35       0.70      0.73      0.71       300\n          36       0.74      0.78      0.76       300\n          37       0.97      0.98      0.98       300\n\n    accuracy                           0.75     11400\n   macro avg       0.75      0.75      0.75     11400\nweighted avg       0.75      0.75      0.75     11400\n\n0.7483333333333333\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(20, 10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=label_encoder.classes_, \n            yticklabels=label_encoder.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T02:07:29.737313Z","iopub.execute_input":"2025-01-23T02:07:29.737671Z","iopub.status.idle":"2025-01-23T02:07:29.820077Z","shell.execute_reply.started":"2025-01-23T02:07:29.737641Z","shell.execute_reply":"2025-01-23T02:07:29.818645Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-453ff3827965>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n\u001b[1;32m      4\u001b[0m             \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             yticklabels=label_encoder.classes_)\n","\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"],"ename":"NameError","evalue":"name 'y_test' is not defined","output_type":"error"}],"execution_count":5}]}